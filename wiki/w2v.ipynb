{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка к работе с Википедией\n",
    "\n",
    "Скачайте 'enwiki-20200401-pages-articles.xml.bz2' по ссылке https://meta.wikimedia.org/wiki/Data_dump_torrents — архив весит порядка 16Гб\n",
    "\n",
    "Скачайте 'wiki.corpus' по ссылке https://yadi.sk/d/TVo-KPUbgx4vPA — это слепок памяти объекта для работы с нелемматизированной(!) Википедией\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.wikicorpus import WikiCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging is important to get the state of the functions\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-23 04:38:51,654: INFO: loading WikiCorpus object from wiki.corpus\n",
      "2020-11-23 04:38:53,799: INFO: loading dictionary recursively from wiki.corpus.dictionary.* with mmap=None\n",
      "2020-11-23 04:38:53,800: INFO: loaded wiki.corpus\n"
     ]
    }
   ],
   "source": [
    "wiki = WikiCorpus.load('wiki.corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построим word2vec вручную средствами gensim\n",
    "\n",
    "Использовался код https://gist.github.com/maxbellec/85d90d3d7f2f96589f1517e5c4567dc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-20 19:38:17,061: INFO: collecting all words and their counts\n",
      "C:\\Users\\cheptil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gensim\\utils.py:1268: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected %s; aliasing chunkize to chunkize_serial\" % entity)\n",
      "2020-11-20 19:38:35,687: INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "class MySentences(object):\n",
    "    def __iter__(self):\n",
    "        for text in wiki.get_texts():\n",
    "            yield text\n",
    "sentences = MySentences()\n",
    "params = {'size': 300, 'window': 10, 'min_count': 40, \n",
    "          'workers': max(1, multiprocessing.cpu_count() - 1), 'sample': 1e-3,}\n",
    "word2vec = Word2Vec(sentences, **params)\n",
    "word2vec.save('wiki.word2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель посчиталась, пора оценить её качество\n",
    "\n",
    "Скачать предпосчитанную модель можно по ссылкам, она состоит из трёх частей:\n",
    "\n",
    "- 'wiki.word2vec.model' - https://yadi.sk/d/LTFU0Ukc2Bp2MA\n",
    "\n",
    "- 'wiki.word2vec.model.trainables.syn1neg.npy' - https://yadi.sk/d/g7oWXFwga8l9OA\n",
    "\n",
    "- 'wiki.word2vec.model.wv.vectors.npy' - https://yadi.sk/d/nGaMaQT_FkqnLQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-23 04:39:18,217: INFO: loading Word2Vec object from wiki.word2vec.model\n",
      "2020-11-23 04:39:20,465: INFO: loading wv recursively from wiki.word2vec.model.wv.* with mmap=None\n",
      "2020-11-23 04:39:20,466: INFO: loading vectors from wiki.word2vec.model.wv.vectors.npy with mmap=None\n",
      "2020-11-23 04:39:28,183: INFO: setting ignored attribute vectors_norm to None\n",
      "2020-11-23 04:39:28,185: INFO: loading vocabulary recursively from wiki.word2vec.model.vocabulary.* with mmap=None\n",
      "2020-11-23 04:39:28,186: INFO: loading trainables recursively from wiki.word2vec.model.trainables.* with mmap=None\n",
      "2020-11-23 04:39:28,186: INFO: loading syn1neg from wiki.word2vec.model.trainables.syn1neg.npy with mmap=None\n",
      "2020-11-23 04:39:34,573: INFO: setting ignored attribute cum_table to None\n",
      "2020-11-23 04:39:34,575: INFO: loaded wiki.word2vec.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "642768"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "word2vec = Word2Vec.load('wiki.word2vec.model')\n",
    "len(word2vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "f=open(\"SimLex-999.txt\", 'r').readlines()\n",
    "\n",
    "def rank(model):\n",
    "        \n",
    "    not_in_model=[]\n",
    "    w2v_pairs=[]\n",
    "    for i in f[1:]:\n",
    "        ii=i.split('\\t')\n",
    "        first_word=ii[0]\n",
    "        second_word=ii[1]\n",
    "        flag=0\n",
    "        if first_word not in model:\n",
    "            not_in_model.append(first_word.split('_')[0])\n",
    "            flag=1\n",
    "        if second_word not in model:\n",
    "            not_in_model.append(second_word.split('_')[0])\n",
    "            flag=1\n",
    "        if not flag:\n",
    "            w2v_pairs.append(model.distance(first_word, second_word))\n",
    "        #print(first_word, second_word)\n",
    "    print(len(w2v_pairs), not_in_model)\n",
    "    \n",
    "    simlex_pairs=[]\n",
    "    for i in f[1:]:\n",
    "        ii=i.split('\\t')\n",
    "        if ii[0] not in not_in_model and ii[1] not in not_in_model:\n",
    "            simlex_pairs.append(float(ii[3]))\n",
    "    print(len(simlex_pairs))\n",
    "    \n",
    "    return spearmanr(simlex_pairs, w2v_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999 []\n",
      "999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=-0.36182557483841277, pvalue=2.9039845974509423e-32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank(word2vec.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## А теперь построим w2v для лемматизированной Вики\n",
    "\n",
    "Скачайте 'wiki.lem.corpus' по ссылке https://yadi.sk/d/AsaBf1j_oFBFHw — это слепок памяти объекта для работы с лемматизированной(!) Википедией\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-17 15:51:53,758: INFO: loading WikiCorpus object from wiki.lem.corpus\n",
      "2020-12-17 15:51:55,464: INFO: loading dictionary recursively from wiki.lem.corpus.dictionary.* with mmap=None\n",
      "2020-12-17 15:51:55,465: INFO: loaded wiki.lem.corpus\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "\n",
    "wiki = WikiCorpus.load('wiki.lem.corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "class MySentences(object):\n",
    "    def __iter__(self):\n",
    "        for text in wiki.get_texts():\n",
    "            yield [word.decode('utf-8').split('/')[0] for word in text]\n",
    "sentences = MySentences()\n",
    "params = {'size': 300, 'window': 10, 'min_count': 40, \n",
    "          'workers': max(1, multiprocessing.cpu_count() - 1), 'sample': 1e-3,}\n",
    "word2vec = Word2Vec(sentences, **params)\n",
    "word2vec.save('wiki.lem.word2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель посчиталась, пора оценить её качество\n",
    "\n",
    "Скачать предпосчитанную модель можно по ссылкам, она состоит из трёх частей:\n",
    "\n",
    "- 'wiki.lem.word2vec.model' - https://yadi.sk/d/dxyelACAqAVQ0w\n",
    "\n",
    "- 'wiki.lem.word2vec.model.trainables.syn1neg.npy' - https://yadi.sk/d/aE9Ak2-SZrg5tQ\n",
    "\n",
    "- 'wiki.lem.word2vec.model.wv.vectors.npy' - https://yadi.sk/d/In6x505G-JCPSg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-17 15:53:38,177: INFO: loading Word2Vec object from wiki.lem.word2vec.model\n",
      "2020-12-17 15:53:39,716: INFO: loading wv recursively from wiki.lem.word2vec.model.wv.* with mmap=None\n",
      "2020-12-17 15:53:39,717: INFO: loading vectors from wiki.lem.word2vec.model.wv.vectors.npy with mmap=None\n",
      "2020-12-17 15:53:40,507: INFO: setting ignored attribute vectors_norm to None\n",
      "2020-12-17 15:53:40,508: INFO: loading vocabulary recursively from wiki.lem.word2vec.model.vocabulary.* with mmap=None\n",
      "2020-12-17 15:53:40,533: INFO: loading trainables recursively from wiki.lem.word2vec.model.trainables.* with mmap=None\n",
      "2020-12-17 15:53:40,534: INFO: loading syn1neg from wiki.lem.word2vec.model.trainables.syn1neg.npy with mmap=None\n",
      "2020-12-17 15:53:41,320: INFO: setting ignored attribute cum_table to None\n",
      "2020-12-17 15:53:41,321: INFO: loaded wiki.lem.word2vec.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "586969"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "word2vec = Word2Vec.load('wiki.lem.word2vec.model')\n",
    "len(word2vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "f=open(\"SimLex-999.txt\", 'r').readlines()\n",
    "\n",
    "def rank(model):        \n",
    "    w2v_pairs=[]\n",
    "    simlex_pairs=[]\n",
    "    for i in f[1:]:\n",
    "        ii=i.split('\\t')\n",
    "        first_word=ii[0]\n",
    "        second_word=ii[1]\n",
    "        if first_word in model and second_word in model:\n",
    "            w2v_pairs.append(model.distance(first_word, second_word))\n",
    "            simlex_pairs.append(float(ii[3]))\n",
    "    print(len(w2v_pairs))\n",
    "    return spearmanr(simlex_pairs, w2v_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=-0.3901066811150612, pvalue=2.679880250653796e-37)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank(word2vec.wv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
