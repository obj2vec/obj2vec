# obj2vec

Проект посвящён построению векторных представлений объектов по их близостям.

## Задачи и приложения:

0. Изначальная задача заключается в том, чтобы по матрице близостей множества объектов получить векторные представления объектов, наилучшим образом соответствущие близостям между объектами — это известные задачи cMDS, mMDS, nMDS (https://en.wikipedia.org/wiki/Multidimensional_scaling, https://www.kaggle.com/c/nicn2/overview/approaches).

Очень важно, что задача nMDS имеет точное решение(ссылки ниже) - однако про размерность представлений ничего не сказано, предположительно, верхняя оценка не должна превосходить количество объектов(см. раздел Теоретические вопросы).

Sameer Agarwal etc. “Generalized Non-metric Multidimensional Scaling”. Proceedings of the
Eleventh International Conference on Artificial Intelligence and Statistics, PMLR 2:11-18, 2007 http://proceedings.mlr.press/v2/agarwal07a/agarwal07a.pdf

Bronstein Alexander M. etc. «Generalized Multidimensional Scaling: A Framework for Isometry-
Invariant Partial Surface Matching». Proceedings of the National Academy of Sciences, 2006,
1168–72. https://www.pnas.org/content/pnas/103/5/1168.full.pdf

Также важно, что nMDS переводит близости в расстояния - т.е. объекты становятся элементами пространства(множество объектов и функция расстояния), при этом если выполнено неравенство треугольника, то пространство является метрическим, а расстояние метрикой. Это важно, поскольку есть гипотеза, что в метрических пространствах поиск ближайших соседей может осуществляться быстрее, чем в неметрических пространствах и не пространствах(см. раздел Идеи для статей). 

Наконец, существенным является замечание, что в метрическом простанстве матрица расстояний эквивалентна векторным представлениям, поскольку матрица и представления однозначно преобразуются друг в друга(тут будет ссылка).

Приложением nMDS является понижение размерности - для представлений высокой размерности строится матрица близостей, которая затем вкладывается в пространство меньшей размерности.



## План статей:

I. Схема валидации и понижение размерности

У эмбеддингов есть 4 важных свойства: семантической близости, аналогий, интерпретируемость, метричность
 
Для каждого свойства описывается способ, как оценить величину соблюдения этого свойства для заданных эмбеддингов
 
Затем говорится, что есть средство понижения размерности Ivis, которое принимает на вход некоторые эмбеддинги
 
Мы находим такие настройки параметра средства Ivis, что для них валидационная схема выдаёт максимальные показатели качества выбранных свойств
 
 
II. Свой способ построения эмбеддингов методами MDS
 
Идея в том, что обычно берут сырые тексты, строят на них представления для слов и затем считают матрицу попарных расстояний, а можно из сырых текстов посчитать матрицу попарных расстояний и затем построить представления по ней
 
Кажется, что это новый подход к построению эмбеддингов, но если такая же последовательность шагов уже известна, то в любом случае реализация каждого из них может быть новой — обзор подходов к построению эмбеддингов нужен в любом случае
 
Первый шаг предлагается делать так — для каждого слова берётся множество его контекстов, получает мешок контекстов, псевдодокумент слова, для каждый двух мешков считается коэффициент Жаккара, так получается матрица попарных близостей, для неё тоже нужна схема валидации, предлагается использовать размеченный экспертами набор близостей пар слов и смотреть отклонение к.к.Спирмена от 1
 
Второй шаг — точное nMDS, затем понижение размерности силами результатов из I.


Подходы:

Педставляет интерес перестановочный метод, но также интересен и графовый метод, когда в пространстве размещается граф, полученный по матрице близостей


III. Использование результатов II. как первый слой в контекстно-зависимых моделях

http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/


## Литература:

Знакомство с предметной областью — https://ru.wikipedia.org/wiki/Дистрибутивная_семантика
 
С чего всё началось — http://www.jmlr.org/papers/v3/bengio03a.html

Efficient Estimation of Word Representations in Vector Space - https://arxiv.org/abs/1301.3781

Как с этим работать — https://github.com/akutuzov/webvectors/blob/master/preprocessing/rusvectores_tutorial.ipynb

Примеры с кодом по оцениванию представлений (рагновая корелляция и решение задачи аналогий): https://nlp.gluon.ai/examples/word_embedding_evaluation/word_embedding_evaluation.html

GluonCV and GluonNLP: Deep Learning in
Computer Vision and Natural Language Processing - https://www.jmlr.org/papers/volume21/19-429/19-429.pdf

Ensembling Context-Free and Context-Dependent Word Representations:
https://arxiv.org/pdf/2005.06602.pdf



## Идеи для статей:

I. Быстрый поиск ближайших соседей при переходе в метрическое пространство(nMDS+VP-tree)


## Теоретические вопросы:

I. Верхняя оценка на размерность представлений при точном решении задачи nMDS

