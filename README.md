# obj2vec

Проект посвящён построению векторных представлений объектов по близостям между объектами.

## Задачи и приложения:

### Существующие задачи

Изначальная задача заключается в том, чтобы по матрице близостей множества объектов получить векторные представления объектов, наилучшим образом соответствущие близостям между объектами — это известные задачи многомерного шкалирования: cMDS, mMDS, nMDS (https://en.wikipedia.org/wiki/Multidimensional_scaling, https://www.kaggle.com/c/nicn2/overview/approaches).

Очень важно, что задача nMDS имеет точное решение(ссылки ниже) - однако про размерность представлений ничего не сказано, предположительно, верхняя оценка не должна превосходить количество объектов(см. раздел Теоретические вопросы).

Sameer Agarwal etc. “Generalized Non-metric Multidimensional Scaling”. Proceedings of the
Eleventh International Conference on Artificial Intelligence and Statistics, PMLR 2:11-18, 2007 http://proceedings.mlr.press/v2/agarwal07a/agarwal07a.pdf

Bronstein Alexander M. etc. «Generalized Multidimensional Scaling: A Framework for Isometry-
Invariant Partial Surface Matching». Proceedings of the National Academy of Sciences, 2006,
1168–72. https://www.pnas.org/content/pnas/103/5/1168.full.pdf

Также важно, что nMDS переводит близости в расстояния - т.е. объекты становятся элементами пространства(множество объектов и функция расстояния), при этом если выполнено неравенство треугольника, то пространство является метрическим, а расстояние метрикой. Это важно, поскольку есть гипотеза, что в метрических пространствах поиск ближайших соседей может осуществляться быстрее, чем в неметрических пространствах и не пространствах(см. раздел Идеи для статей). 

Наконец, существенным является замечание, что в метрическом простанстве матрица расстояний эквивалентна векторным представлениям, поскольку матрица и представления однозначно преобразуются друг в друга(тут будет ссылка).

Приложением nMDS является понижение размерности - для представлений высокой размерности строится матрица близостей, которая затем вкладывается в пространство меньшей размерности. Примером из жизни может служить задача, когда набор трёхмерных координат городов нужно отобразить на плоскую карту, сохранив монотоность расстояний между городами. Также интересно, что с помощью cMDS можно доказать, что Земля не плоская.

### Новые задачи

I. Задачи, описанные в этом и следующих пунктах, основаны на идее, что числа, составляющие матрицу близостей имеют смысл сходства(близость это математическое понятие — неотрицательная несимметричная функция двух аргументов; сходство это психологическое понятие, характеризующее похожесть образом в сознании). Для трёх объектов эта связь близости и сходства означает, что если близость между A и B меньше чем между A и C, то A больше похож по смыслу на B, чем на C. Для четырёх объектов это означает, что если близость между A и B меньше чем между C и D, то A больше похож по смыслу на B, чем C на D.

Очень важно прочуствовать различие между математической близостью и психологическим сходством. Так, в привёденном ранее примере с городами из того, что города находятся рядом географически не следует, что они похожи в сознании граждан, поскольку эти города могут находиться в разных странах, располагаясь возле границы. И наоборот, далекие в пространстве города могут быть похожи в сознании путешественников.

Содержательно, эта идея приводит к возможности добавлять внешнюю информацию. Например, эксперт, у которого в сознании есть понятие сходства может выразить его численно в виде близости(точно также как преподаватели оценивают знания студентов на экзаменах, только в данном случае речь идёт о парах объектов). Начиная отсюда и далее, если особо не сказано иное, считается, что близость и сходство взаимозаменяемые понятия(почему? сходство в сознании, а близость это число), поскольку вводится допущение, что **похожие по смыслу объекты близки**(нужна грамотная формулировка, учитывающая все четыре варианта).

Используя предподсчитанные векторные представления объектов можно получить матрицу близостей. Затем можно взять экспертные значения близостей, т.е. внешнюю информацию, и заменить ими значения в соответствующих ячейках исходной матрицы близостей. Решив задачу ММШ для полученной матрицы, можно получить векторные представления более высокого качества. Для cMDS и mMDS качество представлений измеряется с помощью корреляции Пирсона с некоторыми эталонными значениями близостей. Для nMDS используется корреляция Спирмена, с рангами эталонных значений близостей(могут ли быть эталонные ранги?).

Приложением данной задачи является повышение качества предподсчитанных векторных представлений. Стоит отметить, что размерность представлений не меняется, что означает, что делает подход прозрачным для применения в существующих применениях векторных представлений.



## План статей:

I. Схема валидации и понижение размерности

У эмбеддингов есть 4 важных свойства: семантической близости, аналогий, интерпретируемость, метричность
 
Для каждого свойства описывается способ, как оценить величину соблюдения этого свойства для заданных эмбеддингов
 
Затем говорится, что есть средство понижения размерности Ivis, которое принимает на вход некоторые эмбеддинги
 
Мы находим такие настройки параметра средства Ivis, что для них валидационная схема выдаёт максимальные показатели качества выбранных свойств
 
 
II. Свой способ построения эмбеддингов методами MDS
 
Идея в том, что обычно берут сырые тексты, строят на них представления для слов и затем считают матрицу попарных расстояний, а можно из сырых текстов посчитать матрицу попарных расстояний и затем построить представления по ней
 
Кажется, что это новый подход к построению эмбеддингов, но если такая же последовательность шагов уже известна, то в любом случае реализация каждого из них может быть новой — обзор подходов к построению эмбеддингов нужен в любом случае
 
Первый шаг предлагается делать так — для каждого слова берётся множество его контекстов, получает мешок контекстов, псевдодокумент слова, для каждый двух мешков считается коэффициент Жаккара, так получается матрица попарных близостей, для неё тоже нужна схема валидации, предлагается использовать размеченный экспертами набор близостей пар слов и смотреть отклонение к.к.Спирмена от 1
 
Второй шаг — точное nMDS, затем понижение размерности силами результатов из I.


Подходы:

Педставляет интерес перестановочный метод, но также интересен и графовый метод, когда в пространстве размещается граф, полученный по матрице близостей


III. Использование результатов II. как первый слой в контекстно-зависимых моделях

http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/


## Литература:

Знакомство с предметной областью — https://ru.wikipedia.org/wiki/Дистрибутивная_семантика
 
С чего всё началось — http://www.jmlr.org/papers/v3/bengio03a.html

Efficient Estimation of Word Representations in Vector Space - https://arxiv.org/abs/1301.3781

Как с этим работать — https://github.com/akutuzov/webvectors/blob/master/preprocessing/rusvectores_tutorial.ipynb

Примеры с кодом по оцениванию представлений (рагновая корелляция и решение задачи аналогий): https://nlp.gluon.ai/examples/word_embedding_evaluation/word_embedding_evaluation.html

GluonCV and GluonNLP: Deep Learning in
Computer Vision and Natural Language Processing - https://www.jmlr.org/papers/volume21/19-429/19-429.pdf

Ensembling Context-Free and Context-Dependent Word Representations:
https://arxiv.org/pdf/2005.06602.pdf



## Идеи для статей:

I. Быстрый поиск ближайших соседей при переходе в метрическое пространство(nMDS+VP-tree)


## Теоретические вопросы:

I. Верхняя оценка на размерность представлений при точном решении задачи nMDS

